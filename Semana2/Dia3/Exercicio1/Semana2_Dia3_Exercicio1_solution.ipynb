{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9995f8",
   "metadata": {},
   "source": [
    "# FAQ para Suporte Interno FAISS, LangChain e Ollama (llama3.2)\n",
    "* **Descrição:** Criar um script base em Python, capaz de ler o arquivo com perguntas e respostas sobre a empresa, indexar as informações no FAISS DB, fazer Q&A com o ollama, usando LangChain para orquestrar tudo\n",
    "* **Dataset:** company-info.json incluído no projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f129db9",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "1. Puxar um modelo do llama usando ollama (ex: llama3.2)\n",
    "2. Servir localmente o modelo usando\n",
    "```bash\n",
    "    ollama serve\n",
    "```\n",
    "3. Carregar as perguntas e respostas do JSON\n",
    "4. Indexar no FAISS a resposta como conteúdo e a pergunta \n",
    "5. Criar as peças para o chain (embeding, retriever)\n",
    "6. Montar o chain\n",
    "7. Executar pesquisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar dependências\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_community.llms import Ollama          # LLM local\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8011613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Carregar os dados do json\n",
    "with open('company-info.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "# Converter os dados em Documents para indexação\n",
    "documents = []\n",
    "for item in data:\n",
    "    # Usamos a resposta como conteúdo e a pergunta como metadados\n",
    "    doc = Document(\n",
    "        page_content=item['answer'],\n",
    "        metadata={'question': item['question']}\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"Carregados {len(documents)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Criar OllamaEmbeddings com o modelo que vai usar (vai usar localmente)\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "print(\"Embeddings configurado com modelo llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc474846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Indexar dados com FAISS\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "print(\"Documentos indexados com sucesso no FAISS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Definir retriever que vai puxar os dados\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retorna os 3 documentos mais relevantes\n",
    ")\n",
    "print(\"Retriever configurado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Configurar o LLM e Chain\n",
    "# Configurar o modelo Ollama local\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Definir o template de prompt para o RAG\n",
    "prompt_template = \"\"\"\n",
    "Use the following context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Criar o chain de RAG\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "print(\"LLM e Chain configurados com sucesso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    # 6. Desenvolver a função de Q&A que usa o chain\n",
    "    print(f\"\\nPergunta: {question}\\n\")\n",
    "    \n",
    "    # Executa a consulta no chain\n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    # Mostrar a resposta\n",
    "    print(\"Resposta:\")\n",
    "    print(result[\"result\"])\n",
    "    \n",
    "    # Mostrar os documentos que foram usados para responder a pergunta\n",
    "    print(\"\\nDocumentos usados para a resposta:\")\n",
    "    for i, doc in enumerate(result[\"source_documents\"]):\n",
    "        print(f\"\\nDocumento {i+1}:\")\n",
    "        print(f\"Conteúdo: {doc.page_content}\")\n",
    "        print(f\"Pergunta original: {doc.metadata['question']}\")\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "ask(\"What services does Capgemini offer?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
