{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9995f8",
   "metadata": {},
   "source": [
    "# FAQ para Suporte Interno FAISS, LangChain e Ollama (llama3.2)\n",
    "* **Descrição:** Criar um script base em Python, capaz de ler o arquivo com perguntas e respostas sobre a empresa, indexar as informações no FAISS DB, fazer Q&A com o ollama, usando LangChain para orquestrar tudo\n",
    "* **Dataset:** company-info.json incluído no projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f129db9",
   "metadata": {},
   "source": [
    "## Passo a passo\n",
    "1. Puxar um modelo do llama usando ollama (ex: llama3.2)\n",
    "2. Servir localmente o modelo usando\n",
    "```bash\n",
    "    ollama serve\n",
    "```\n",
    "3. Carregar as perguntas e respostas do JSON\n",
    "4. Indexar no FAISS a resposta como conteúdo e a pergunta \n",
    "5. Criar as peças para o chain (embeding, retriever)\n",
    "6. Montar o chain\n",
    "7. Executar pesquisas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b3c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar dependências\n",
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama          # LLM local\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8011613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. Carregar os dados do json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2. Criar OllamaEmbeddings com o modelo que vai usar (vai usar localmente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc474846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 3. Indexar dados com FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 4. Definir retriever que vai puxar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5. Configurar o LLM e Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    #TODO: 6. Desenvolver a função de Q&A que usa o chain\n",
    "    # Mostrar a resposta\n",
    "    # Mostrar os documentos que foram usados para responder a pergunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "ask(\"What services does Capgemini offer?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
