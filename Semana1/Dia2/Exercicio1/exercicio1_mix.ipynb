{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício de geração de texto\n",
        "Usando a livraria ***transformers*** e modelos de língua portuguesa, hospedados pela Hugging Face\n",
        "\n",
        "## Requisitos de Sistema\n",
        "\n",
        "*   RAM: 32 GB\n",
        "*   Disco: 60 GB\n",
        "\n",
        "NOTA: Executando exemplo em máquina com GPU A100 usou 4.6 GB RAM de Sistema e 26.6 GB RAM do GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "1eu-XBxb0zDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependências\n",
        "Para executar localmente instalar primeiro as bibliotecas transformers e o torch\n",
        "```\n",
        "# NOTA: Antes é recomendado que crie e ative um virtual env\n",
        "# instalar bibliotecas transformers e torch\n",
        "pip install transformers torch\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "-MYvAQCkwFVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "print(\"Vamos fazer um exercício de geração de texto, baseado em um tópico\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHEwtkrT2XJW",
        "outputId": "472012d7-7683-4fbd-d440-2014424bb574"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vamos fazer um exercício de geração de texto, baseado em um tópico\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTA: Se executar mais de uma vez ele tenta carregar novamente o modelo em memória!!!\n",
        "# Inicializar pipeline de geração com modelo GPT-2\n",
        "generator1 = pipeline('text-generation', model='pierreguillou/gpt2-small-portuguese')\n",
        "# Inicializar pipeline de geração com modelo Gervasio\n",
        "generator2 = pipeline('text-generation', model='PORTULAN/gervasio-7b-portuguese-ptpt-decoder')"
      ],
      "metadata": {
        "id": "1xGS2kAU2dER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dLQEC4uvUsz"
      },
      "outputs": [],
      "source": [
        "# Pedir um tópico para o utilizador\n",
        "prompt = input(\"Digite um tópico: \")\n",
        "\n",
        "# Gerar Usando GPT-2 PT\n",
        "resultado_gpt = generator1(prompt, max_length=100, truncation=False, num_return_sequences=1, temperature=0.5)\n",
        "print(\"\\n\\nContinuação gerada (GPT-2 PT):\", resultado_gpt[0]['generated_text'])\n",
        "\n",
        "# Gerar Usando Gervasio PT\n",
        "resultado_gervasio = generator2(prompt, max_length=100, truncation=False, num_return_sequences=1, temperature=0.5)\n",
        "print(\"\\n\\nContinuação gerada (Gervasio PT):\", resultado_gervasio[0]['generated_text'])"
      ]
    }
  ]
}