{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eu-XBxb0zDj"
      },
      "source": [
        "# Exercício de geração de texto\n",
        "Usando a livraria ***transformers*** e modelos de língua portuguesa, hospedados pela Hugging Face\n",
        "\n",
        "## Requisitos de Sistema\n",
        "\n",
        "*   RAM: 32 GB\n",
        "*   Disco: 60 GB\n",
        "\n",
        "NOTA: Executando exemplo em máquina com GPU A100 usou 4.6 GB RAM de Sistema e 26.6 GB RAM do GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYvAQCkwFVH"
      },
      "source": [
        "## Dependências\n",
        "Para executar localmente instalar primeiro as bibliotecas transformers e o torch\n",
        "```\n",
        "# NOTA: Antes é recomendado que crie e ative um virtual env\n",
        "# instalar bibliotecas transformers e torch\n",
        "pip install transformers torch\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHEwtkrT2XJW",
        "outputId": "c3bbb202-7d2c-46f7-d89f-3b2ff15bde66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vamos fazer um exercício de geração de texto, baseado em um tópico\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "print(\"Vamos fazer um exercício de geração de texto, baseado em um tópico\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xGS2kAU2dER"
      },
      "outputs": [],
      "source": [
        "# NOTA: Se executar mais de uma vez ele tenta carregar novamente o modelo em memória!!!\n",
        "# Inicializar pipeline de geração com modelo GPT-2\n",
        "generator1 = pipeline('text-generation', model='pierreguillou/gpt2-small-portuguese')\n",
        "# Inicializar pipeline de geração com modelo Gervasio\n",
        "generator2 = pipeline('text-generation', model='PORTULAN/gervasio-7b-portuguese-ptpt-decoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dLQEC4uvUsz"
      },
      "outputs": [],
      "source": [
        "# Pedir um tópico para o utilizador\n",
        "prompt = input(\"Digite um tópico: \")\n",
        "\n",
        "# Gerar Usando GPT-2 PT\n",
        "resultado_gpt = generator1(prompt, max_length=100, truncation=False, num_return_sequences=1, temperature=0.5)\n",
        "print(\"\\n\\nContinuação gerada (GPT-2 PT):\", resultado_gpt[0]['generated_text'])\n",
        "\n",
        "# Gerar Usando Gervasio PT\n",
        "resultado_gervasio = generator2(prompt, max_length=100, truncation=False, num_return_sequences=1, temperature=0.5)\n",
        "print(\"\\n\\nContinuação gerada (Gervasio PT):\", resultado_gervasio[0]['generated_text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Acessar um modelo mais avançado\n",
        "* Para Confirgurar um token HF: https://pyimagesearch.com/2025/04/04/configure-your-hugging-face-access-token-in-colab-environment\n",
        "* Para solicitar acesso ao modelo, tem de ir na página do modelo e solicitar acesso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWX5JVC2-0kW"
      },
      "outputs": [],
      "source": [
        "# LLAMA - Necessita de configurar um token HF e solicitar acesso ao pessoal do LLAMA lá no Hugging Face\n",
        "generator3 = pipeline('text-generation', model='meta-llama/Meta-Llama-3-70B-Instruct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhD1JbHY-3m7"
      },
      "outputs": [],
      "source": [
        "# Gerar Usando LLAMA 3.1 70B\n",
        "resultado_llama = generator3(prompt, max_length=100, truncation=False, num_return_sequences=1, temperature=0.5)\n",
        "print(\"\\n\\nContinuação gerada (LLAMA 3.1):\", resultado_llama[0]['generated_text'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
