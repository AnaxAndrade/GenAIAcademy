{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício: Descoberta de Tópicos em E-mails Corporativos\n",
        "* **Descrição:** Extraia 5 temas dominantes nos e-mails da Enron para obter visão geral dos assuntos tratados internamente.\n",
        "* **Dataset:** Enron Email Dataset (500 k mensagens de email de uma empresa real, que foi publicada enquanto a empresa estava sendo investigada)\n",
        "  * **Mais Informações:** https://www.cs.cmu.edu/~enron/\n",
        "  * **Download:** https://www.kaggle.com/datasets/wcukierski/enron-email-dataset"
      ],
      "metadata": {
        "id": "SfXg_ic15COG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo a passo\n",
        "1. Descarregar emails.csv (colunas message, subject, date, user).\n",
        "2. Filtrar apenas “sent” e “inbox”; amostrar 20 000 linhas para caber em RAM de portátil.\n",
        "3. Pré-processar: minúsculas, remover URLs, stop-words, lematizar (spacy-pt).\n",
        "4. CountVectorizer → matriz BoW (max_features=20 000).\n",
        "5. LatentDirichletAllocation(n_components=5, max_iter=10); ajustar modelo.\n",
        "6. Mostrar top-10 palavras de cada tópico e atribuir rótulos de negócio.\n",
        "7. Calcular distribuição de tópicos por utilizador e visualizar em gráfico de barras."
      ],
      "metadata": {
        "id": "PPl_WO6H5Z7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download do Dataset\n",
        "* Para facilitar, fiz o upload do arquivo .csv no Google drive, link: https://drive.google.com/file/d/1FiIb0dliKf18NsbSCZmz27xXy0MXV8Iq/view\n",
        "* Para fazer download é só executar o comando mais abaixo\n",
        "* Caso o link ou o comando abaixo não funcionem, tem de fazer o download direto do próprio kaggle"
      ],
      "metadata": {
        "id": "rgGl2wm69x0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Opcional: Se quizer fazer o download do arquivo direto do meu Google Drive\n",
        "\n",
        "# NOTA: Em ambiente do Google colab o gdown já funciona diretamente\n",
        "# caso necessário pode instalá-lo via pip, descomentanto a linha abaixo:\n",
        "# !pip install gdown\n",
        "\n",
        "!gdown 1FiIb0dliKf18NsbSCZmz27xXy0MXV8Iq -O emails.csv"
      ],
      "metadata": {
        "id": "gQtjRBYS-Nj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependências\n",
        "#!pip install pandas scikit-learn spacy==3.7.2 spacy-lookups-data pyLDAvis\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "utyg_sf15yFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re, spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation"
      ],
      "metadata": {
        "id": "ouH9xetb53Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. carregar\n",
        "# NOTA: O dataset completo inclui mais de 500 mil emails\n",
        "# Para uma análise inicial pode ser interessante carregar uma amostra menor\n",
        "# por questões de tempo e performance\n",
        "amostra = 20000\n",
        "\n",
        "df = pd.read_csv('emails.csv')\n",
        "df = df[df['file'].str.contains('sent|inbox', na=False)].sample(amostra, random_state=42)"
      ],
      "metadata": {
        "id": "HZ0r-M2a572c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento\n",
        "* Utilizaremos regex para tirar links do texto do email e conver ter para minúscula\n",
        "* Utilizaremos um modelo de linguagem da spacy para fazer limpezas, tratamentos e anotacões no texto de cada email\n",
        "* Chamar nlp(txt) processa o texto de entrada txt usando o pipeline do spaCy carregado. Tokeniza o texto, realiza anotações linguísticas (exceto NER e parsing, que foram desativados) e retorna um objeto Doc contendo esta informação processada. Este objeto Doc pode então ser usado para extrair as features desejadas, como lemas para modelagem de tópicos."
      ],
      "metadata": {
        "id": "1icQjtVWHh3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2–3. pré-processamento\n",
        "nlp = spacy.load('pt_core_news_sm', disable=['ner', 'parser'])\n",
        "def limpa(txt):\n",
        "    txt = re.sub(r'https?\\S+', '', str(txt).lower())\n",
        "    doc = nlp(txt)\n",
        "    tokens = [t.lemma_ for t in doc if t.is_alpha and not t.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "df['clean'] = df['message'].apply(limpa)"
      ],
      "metadata": {
        "id": "0SfwztcF5_qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. vectorização\n",
        "vec = CountVectorizer(max_features=20000)\n",
        "X = vec.fit_transform(df['clean'])"
      ],
      "metadata": {
        "id": "bxugR3LW6ArM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. LDA\n",
        "lda = LatentDirichletAllocation(n_components=5, max_iter=10, learning_method='online', random_state=0)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "jZsUPDzz6EBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. top palavras por tópico\n",
        "palavras = vec.get_feature_names_out()\n",
        "for k, comp in enumerate(lda.components_):\n",
        "    print(f'\\nTópico {k}:', ', '.join(palavras[i] for i in comp.argsort()[-10:][::-1]))"
      ],
      "metadata": {
        "id": "Iv6GHI8e6F_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pontos Adicionais Sugeridos\n",
        "\n",
        "\n",
        "1.   Refretir e entender sobre a limpeza dos dados. Visualizar os dados antes e depois da limpeza\n",
        "2.   Como adaptarias esse código para um outro caso de uso real?\n",
        "\n"
      ],
      "metadata": {
        "id": "nSRABkI56KhX"
      }
    }
  ]
}