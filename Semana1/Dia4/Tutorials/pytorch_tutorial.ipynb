{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de PyTorch: Conceitos e APIs Principais\n",
    "\n",
    "Este tutorial oferece uma visão geral concisa mas completa do PyTorch, uma das bibliotecas mais populares para aprendizagem de máquina e redes neurais. Vamos explorar os componentes principais, APIs e exemplos práticos para tarefas comuns.\n",
    "\n",
    "## Conteúdo\n",
    "1. [Introdução ao PyTorch](#1-introdução-ao-pytorch)\n",
    "2. [Tensores: O Bloco Fundamental](#2-tensores-o-bloco-fundamental)\n",
    "3. [Autograd: Diferenciação Automática](#3-autograd-diferenciação-automática)\n",
    "4. [Construindo Redes Neurais](#4-construindo-redes-neurais)\n",
    "5. [Otimizadores e Funções de Perda](#5-otimizadores-e-funções-de-perda)\n",
    "6. [Carregamento e Processamento de Dados](#6-carregamento-e-processamento-de-dados)\n",
    "7. [Treinamento e Avaliação de Modelos](#7-treinamento-e-avaliação-de-modelos)\n",
    "8. [Salvando e Carregando Modelos](#8-salvando-e-carregando-modelos)\n",
    "9. [Redes Neurais Convolucionais (CNNs)](#9-redes-neurais-convolucionais-cnns)\n",
    "10. [Redes Neurais Recorrentes (RNNs)](#10-redes-neurais-recorrentes-rnns)\n",
    "11. [Processamento de Linguagem Natural](#11-processamento-de-linguagem-natural)\n",
    "12. [Transferência de aprendizagem](#12-transferência-de-aprendizagem)\n",
    "13. [PyTorch para Produção](#13-pytorch-para-produção)\n",
    "14. [Recursos Adicionais](#14-recursos-adicionais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução ao PyTorch\n",
    "\n",
    "PyTorch é uma biblioteca de aprendizagem profundo de código aberto desenvolvida pelo Facebook (Meta). Suas principais características incluem:\n",
    "\n",
    "- **Computação de Tensores**: Similar ao NumPy, mas com suporte a GPUs\n",
    "- **Grafos Dinâmicos**: Define grafos computacionais em tempo de execução (diferente do TensorFlow estático original)\n",
    "- **API Pythônica**: Integração natural com o ecossistema Python\n",
    "- **Ecossistema Rico**: Ferramentas para pesquisa, produção e áreas específicas como visão computacional e NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação do PyTorch (descomente se necessário)\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "# Importações básicas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Verificar versão e disponibilidade de GPU\n",
    "print(f\"PyTorch versão: {torch.__version__}\")\n",
    "print(f\"GPU disponível: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensores: O Bloco Fundamental\n",
    "\n",
    "Tensores são estruturas de dados multidimensionais similares aos arrays do NumPy, mas com funcionalidades adicionais para deep learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando tensores\n",
    "# 1. A partir de listas Python\n",
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Tensor de lista: \\n{x}\\n\")\n",
    "\n",
    "# 2. Tensores preenchidos\n",
    "zeros = torch.zeros(2, 3)  # Tensor 2x3 de zeros\n",
    "ones = torch.ones(2, 3)    # Tensor 2x3 de uns\n",
    "rand = torch.rand(2, 3)    # Tensor 2x3 com valores aleatórios entre 0 e 1\n",
    "print(f\"Zeros: \\n{zeros}\\n\")\n",
    "print(f\"Ones: \\n{ones}\\n\")\n",
    "print(f\"Random: \\n{rand}\\n\")\n",
    "\n",
    "# 3. Tensores com valores específicos\n",
    "arange = torch.arange(0, 10, 2)  # Valores de 0 a 10 com passo 2\n",
    "linspace = torch.linspace(0, 10, 5)  # 5 valores igualmente espaçados entre 0 e 10\n",
    "print(f\"Arange: {arange}\")\n",
    "print(f\"Linspace: {linspace}\\n\")\n",
    "\n",
    "# Propriedades dos tensores\n",
    "print(f\"Forma (shape): {x.shape}\")\n",
    "print(f\"Tipo de dados: {x.dtype}\")\n",
    "print(f\"Dispositivo: {x.device}\\n\")\n",
    "\n",
    "# Operações com tensores\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "# Operações aritméticas\n",
    "print(f\"a + b: {a + b}\")\n",
    "print(f\"a * b: {a * b}\")\n",
    "print(f\"Produto escalar: {torch.dot(a, b)}\\n\")\n",
    "\n",
    "# Redimensionamento\n",
    "c = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original: \\n{c}\")\n",
    "print(f\"Reshape: \\n{c.reshape(3, 2)}\")\n",
    "print(f\"View (mesma memória): \\n{c.view(3, 2)}\\n\")\n",
    "\n",
    "# Movendo tensores para GPU (se disponível)\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = x.to(\"cuda\")\n",
    "    print(f\"Tensor na GPU: {x_gpu.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autograd: Diferenciação Automática\n",
    "\n",
    "O sistema de autograd do PyTorch permite o cálculo automático de gradientes, essencial para o treinamento de redes neurais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando tensores com gradientes\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(f\"Tensor x: \\n{x}\\n\")\n",
    "\n",
    "# Realizando operações\n",
    "y = x + 2\n",
    "print(f\"y = x + 2: \\n{y}\\n\")\n",
    "\n",
    "# Operação mais complexa\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(f\"z = 3 * y * y: \\n{z}\\n\")\n",
    "print(f\"out = z.mean(): {out}\\n\")\n",
    "\n",
    "# Calculando gradientes\n",
    "out.backward()  # Equivalente a out.backward(torch.tensor(1.0))\n",
    "\n",
    "# Verificando gradientes calculados\n",
    "print(f\"Gradiente de x: \\n{x.grad}\\n\")\n",
    "\n",
    "# Desativando o rastreamento de gradiente temporariamente\n",
    "with torch.no_grad():\n",
    "    print(f\"Sem rastreamento de gradiente: {x + 2}\")\n",
    "\n",
    "# Ou usando .detach() para criar uma cópia sem gradiente\n",
    "print(f\"Tensor destacado: {x.detach() + 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construindo Redes Neurais\n",
    "\n",
    "O PyTorch oferece o módulo `nn` para construir redes neurais de forma modular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma rede neural simples usando nn.Module\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Camadas da rede\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Camada totalmente conectada (linear)\n",
    "        self.relu = nn.ReLU()  # Função de ativação ReLU\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Camada de saída\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Fluxo de dados através da rede\n",
    "        x = self.fc1(x)  # Primeira camada linear\n",
    "        x = self.relu(x)  # Aplicação da função de ativação\n",
    "        x = self.fc2(x)  # Segunda camada linear\n",
    "        return x\n",
    "\n",
    "# Criando uma instância do modelo\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "output_size = 2\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "print(model)\n",
    "\n",
    "# Criando dados de entrada aleatórios\n",
    "x = torch.randn(5, input_size)  # 5 amostras com 10 características cada\n",
    "\n",
    "# Passando os dados pela rede\n",
    "output = model(x)\n",
    "print(f\"\\nEntrada: {x.shape}\")\n",
    "print(f\"Saída: {output.shape}\")\n",
    "\n",
    "# Usando Sequential para definir redes de forma mais concisa\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(input_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, output_size)\n",
    ")\n",
    "print(f\"\\nModelo Sequential:\\n{model_sequential}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Otimizadores e Funções de Perda\n",
    "\n",
    "O PyTorch fornece vários otimizadores e funções de perda para treinar redes neurais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções de perda comuns\n",
    "# 1. Para classificação\n",
    "cross_entropy = nn.CrossEntropyLoss()  # Combina LogSoftmax e NLLLoss\n",
    "bce = nn.BCELoss()  # Binary Cross Entropy para classificação binária\n",
    "bce_with_logits = nn.BCEWithLogitsLoss()  # BCE com sigmoid integrado\n",
    "\n",
    "# 2. Para regressão\n",
    "mse = nn.MSELoss()  # Erro Quadrático Médio\n",
    "mae = nn.L1Loss()   # Erro Absoluto Médio\n",
    "smooth_l1 = nn.SmoothL1Loss()  # Huber Loss (menos sensível a outliers)\n",
    "\n",
    "# Exemplo de cálculo de perda para classificação\n",
    "outputs = torch.randn(3, 5)  # 3 amostras, 5 classes\n",
    "targets = torch.tensor([1, 0, 4])  # Classes alvo\n",
    "loss = cross_entropy(outputs, targets)\n",
    "print(f\"Perda de entropia cruzada: {loss.item()}\\n\")\n",
    "\n",
    "# Exemplo de cálculo de perda para regressão\n",
    "outputs_reg = torch.randn(3, 1)  # 3 amostras, 1 valor previsto\n",
    "targets_reg = torch.randn(3, 1)  # 3 valores alvo\n",
    "loss_reg = mse(outputs_reg, targets_reg)\n",
    "print(f\"Perda MSE: {loss_reg.item()}\\n\")\n",
    "\n",
    "# Otimizadores\n",
    "# Criando um modelo simples para demonstração\n",
    "model = SimpleNN(10, 20, 5)\n",
    "\n",
    "# 1. SGD (Stochastic Gradient Descent)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 2. Adam (Adaptive Moment Estimation)\n",
    "adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "# 3. RMSprop\n",
    "rmsprop = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.99)\n",
    "\n",
    "# 4. Adagrad\n",
    "adagrad = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "\n",
    "# Exemplo de passo de otimização\n",
    "optimizer = adam  # Escolhendo Adam como otimizador\n",
    "\n",
    "# Fluxo típico de otimização\n",
    "optimizer.zero_grad()  # Zerar gradientes acumulados\n",
    "outputs = model(torch.randn(5, 10))  # Forward pass\n",
    "loss = cross_entropy(outputs, torch.tensor([1, 0, 4, 2, 3]))  # Cálculo da perda\n",
    "loss.backward()  # Backward pass (cálculo de gradientes)\n",
    "optimizer.step()  # Atualização dos parâmetros\n",
    "\n",
    "print(f\"Otimizador: {type(optimizer).__name__}\")\n",
    "print(f\"Taxa de aprendizagem: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Carregamento e Processamento de Dados\n",
    "\n",
    "O PyTorch oferece ferramentas para carregar e processar dados de forma eficiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um conjunto de dados personalizado\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "# Criando dados sintéticos\n",
    "data = torch.randn(100, 10)  # 100 amostras com 10 características\n",
    "targets = torch.randint(0, 5, (100,))  # 100 rótulos entre 0 e 4\n",
    "\n",
    "# Criando o dataset\n",
    "dataset = CustomDataset(data, targets)\n",
    "print(f\"Tamanho do dataset: {len(dataset)}\")\n",
    "print(f\"Primeiro item: {dataset[0][0].shape}, {dataset[0][1]}\\n\")\n",
    "\n",
    "# Criando um DataLoader\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(f\"Número de batches: {len(dataloader)}\")\n",
    "\n",
    "# Iterando sobre o DataLoader\n",
    "for batch_idx, (data, targets) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx+1}: {data.shape}, {targets.shape}\")\n",
    "    if batch_idx == 2:  # Mostrar apenas os primeiros 3 batches\n",
    "        break\n",
    "\n",
    "# Datasets pré-definidos (comentado para não baixar dados)\n",
    "'''\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Transformações para pré-processamento\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Carregando o dataset MNIST\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Criando DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento e Avaliação de Modelos\n",
    "\n",
    "Vamos ver como treinar e avaliar um modelo em PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo um modelo simples para classificação\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Parâmetros\n",
    "input_size = 10\n",
    "hidden_size = 50\n",
    "num_classes = 5\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "# Criando dados sintéticos\n",
    "X = torch.randn(500, input_size)  # 500 amostras\n",
    "y = torch.randint(0, num_classes, (500,))  # 500 rótulos\n",
    "\n",
    "# Dividindo em treino e teste\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Criando datasets e dataloaders\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Inicializando o modelo\n",
    "model = ClassificationModel(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Função de treinamento\n",
    "def train(model, train_loader, criterion, optimizer, device=\"cpu\"):\n",
    "    model.train()  # Modo de treinamento\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zerar gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass e otimização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estatísticas\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Função de avaliação\n",
    "def evaluate(model, test_loader, criterion, device=\"cpu\"):\n",
    "    model.eval()  # Modo de avaliação\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Desativar cálculo de gradientes\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Estatísticas\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Loop de treinamento\n",
    "print(\"Iniciando treinamento...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    print(f\"Época {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Treino - Perda: {train_loss:.4f}, Acurácia: {train_acc:.2f}%\")\n",
    "    print(f\"  Teste  - Perda: {test_loss:.4f}, Acurácia: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Salvando e Carregando Modelos\n",
    "\n",
    "O PyTorch oferece diferentes maneiras de salvar e carregar modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo completo (arquitetura + parâmetros)\n",
    "torch.save(model, 'modelo_completo.pth')\n",
    "\n",
    "# Salvando apenas os parâmetros do modelo (recomendado)\n",
    "torch.save(model.state_dict(), 'modelo_parametros.pth')\n",
    "\n",
    "# Salvando checkpoint (para continuar treinamento)\n",
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': test_loss\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint.pth')\n",
    "\n",
    "# Carregando o modelo completo\n",
    "loaded_model = torch.load('modelo_completo.pth')\n",
    "loaded_model.eval()  # Colocar em modo de avaliação\n",
    "\n",
    "# Carregando apenas os parâmetros (recomendado)\n",
    "new_model = ClassificationModel(input_size, hidden_size, num_classes)\n",
    "new_model.load_state_dict(torch.load('modelo_parametros.pth'))\n",
    "new_model.eval()\n",
    "\n",
    "# Carregando checkpoint\n",
    "checkpoint = torch.load('checkpoint.pth')\n",
    "model_checkpoint = ClassificationModel(input_size, hidden_size, num_classes)\n",
    "model_checkpoint.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_checkpoint = optim.Adam(model_checkpoint.parameters(), lr=learning_rate)\n",
    "optimizer_checkpoint.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print(\"Modelos salvos e carregados com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Redes Neurais Convolucionais (CNNs)\n",
    "\n",
    "As CNNs são especialmente eficazes para processamento de imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma CNN simples\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Camadas convolucionais\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Camadas totalmente conectadas\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 128)  # Para imagens MNIST 28x28\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Camadas convolucionais\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Achatamento para camadas totalmente conectadas\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Camadas totalmente conectadas\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Criando uma instância do modelo\n",
    "cnn_model = SimpleCNN(num_classes=10)\n",
    "print(cnn_model)\n",
    "\n",
    "# Testando com uma imagem aleatória\n",
    "dummy_input = torch.randn(1, 1, 28, 28)  # (batch_size, channels, height, width)\n",
    "output = cnn_model(dummy_input)\n",
    "print(f\"\\nForma da entrada: {dummy_input.shape}\")\n",
    "print(f\"Forma da saída: {output.shape}\")\n",
    "\n",
    "# Principais camadas convolucionais em PyTorch\n",
    "print(\"\\nPrincipais camadas convolucionais:\")\n",
    "print(\"1. nn.Conv2d - Convolução 2D\")\n",
    "print(\"2. nn.MaxPool2d - Pooling máximo 2D\")\n",
    "print(\"3. nn.AvgPool2d - Pooling médio 2D\")\n",
    "print(\"4. nn.BatchNorm2d - Normalização em lote 2D\")\n",
    "print(\"5. nn.Conv1d - Convolução 1D (para sequências)\")\n",
    "print(\"6. nn.Conv3d - Convolução 3D (para dados volumétricos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Redes Neurais Recorrentes (RNNs)\n",
    "\n",
    "As RNNs são projetadas para processar dados sequenciais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma RNN simples\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada RNN\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicialização do estado oculto\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass através da RNN\n",
    "        out, _ = self.rnn(x, h0)  # out: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decodificando o último estado oculto\n",
    "        out = self.fc(out[:, -1, :])  # Usando apenas o último passo de tempo\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Definindo uma LSTM\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicialização do estado oculto e da célula\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass através da LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decodificando o último estado oculto\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Definindo uma GRU\n",
    "class SimpleGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(SimpleGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada GRU\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Inicialização do estado oculto\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass através da GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        \n",
    "        # Decodificando o último estado oculto\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Parâmetros\n",
    "input_size = 10  # Tamanho de cada elemento da sequência\n",
    "hidden_size = 20  # Tamanho do estado oculto\n",
    "num_layers = 2  # Número de camadas empilhadas\n",
    "num_classes = 5  # Número de classes de saída\n",
    "seq_length = 15  # Comprimento da sequência\n",
    "\n",
    "# Criando instâncias dos modelos\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "lstm_model = SimpleLSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "gru_model = SimpleGRU(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Testando com dados aleatórios\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, seq_length, input_size)\n",
    "\n",
    "# Forward pass\n",
    "rnn_out = rnn_model(x)\n",
    "lstm_out = lstm_model(x)\n",
    "gru_out = gru_model(x)\n",
    "\n",
    "print(f\"Forma da entrada: {x.shape}\")\n",
    "print(f\"Forma da saída RNN: {rnn_out.shape}\")\n",
    "print(f\"Forma da saída LSTM: {lstm_out.shape}\")\n",
    "print(f\"Forma da saída GRU: {gru_out.shape}\")\n",
    "\n",
    "print(\"\\nComparação entre RNN, LSTM e GRU:\")\n",
    "print(\"1. RNN: Simples, mas sofre com o problema de gradientes que desaparecem/explodem\")\n",
    "print(\"2. LSTM: Resolve o problema dos gradientes com células de memória e gates\")\n",
    "print(\"3. GRU: Versão simplificada da LSTM, geralmente mais rápida e com desempenho similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Processamento de Linguagem Natural\n",
    "\n",
    "O PyTorch é amplamente utilizado para tarefas de NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de modelo para classificação de texto\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        \n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Camada LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        # text: [batch size, sent len]\n",
    "        \n",
    "        # Aplicando embedding\n",
    "        embedded = self.embedding(text)  # [batch size, sent len, emb dim]\n",
    "        \n",
    "        # Passando pela LSTM\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Se bidirecional, concatenar os estados ocultos finais\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "            \n",
    "        # Aplicando dropout\n",
    "        hidden = self.dropout(hidden)\n",
    "            \n",
    "        # Camada de saída\n",
    "        return self.fc(hidden)\n",
    "\n",
    "# Parâmetros\n",
    "vocab_size = 10000  # Tamanho do vocabulário\n",
    "embedding_dim = 100  # Dimensão do embedding\n",
    "hidden_dim = 256  # Dimensão do estado oculto\n",
    "output_dim = 3  # Número de classes\n",
    "n_layers = 2  # Número de camadas LSTM\n",
    "bidirectional = True  # LSTM bidirecional\n",
    "dropout = 0.5  # Taxa de dropout\n",
    "\n",
    "# Criando o modelo\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout)\n",
    "print(model)\n",
    "\n",
    "# Testando com dados aleatórios\n",
    "batch_size = 4\n",
    "seq_length = 20\n",
    "text = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "output = model(text)\n",
    "\n",
    "print(f\"\\nForma da entrada: {text.shape}\")\n",
    "print(f\"Forma da saída: {output.shape}\")\n",
    "\n",
    "print(\"\\nComponentes comuns para NLP em PyTorch:\")\n",
    "print(\"1. nn.Embedding - Camada de embedding para representar palavras\")\n",
    "print(\"2. nn.LSTM/GRU - Para processar sequências de texto\")\n",
    "print(\"3. nn.Transformer - Para modelos baseados em atenção\")\n",
    "print(\"4. Bibliotecas complementares: torchtext, transformers (Hugging Face)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Transferência de aprendizagem\n",
    "\n",
    "O PyTorch facilita o uso de modelos pré-treinados para transferência de aprendizagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de transferência de aprendizagem com modelos pré-treinados\n",
    "'''\n",
    "import torchvision.models as models\n",
    "\n",
    "# Carregando um modelo ResNet pré-treinado\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Congelando os parâmetros do modelo\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Modificando a camada final para nossa tarefa específica\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 10)  # 10 classes para nossa tarefa\n",
    "\n",
    "# Agora apenas os parâmetros da camada final serão treinados\n",
    "print(\"Modelo ResNet modificado para transferência de aprendizagem:\")\n",
    "print(resnet.fc)\n",
    "'''\n",
    "\n",
    "print(\"Modelos pré-treinados disponíveis em torchvision:\")\n",
    "print(\"1. ResNet (18, 34, 50, 101, 152)\")\n",
    "print(\"2. VGG (11, 13, 16, 19)\")\n",
    "print(\"3. DenseNet (121, 169, 201, 161)\")\n",
    "print(\"4. Inception v3\")\n",
    "print(\"5. MobileNet v2/v3\")\n",
    "print(\"6. EfficientNet\")\n",
    "\n",
    "print(\"\\nEtapas comuns para transferência de aprendizagem:\")\n",
    "print(\"1. Carregar modelo pré-treinado\")\n",
    "print(\"2. Congelar parâmetros (opcional)\")\n",
    "print(\"3. Modificar camadas finais para a nova tarefa\")\n",
    "print(\"4. Treinar o modelo com taxa de aprendizagem reduzida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PyTorch para Produção\n",
    "\n",
    "O PyTorch oferece ferramentas para otimizar e implementar modelos em produção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TorchScript para otimização e portabilidade\n",
    "'''\n",
    "# Convertendo um modelo para TorchScript via tracing\n",
    "example_input = torch.rand(1, 10)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "\n",
    "# Salvando o modelo otimizado\n",
    "traced_model.save(\"traced_model.pt\")\n",
    "\n",
    "# Convertendo um modelo para TorchScript via scripting\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"scripted_model.pt\")\n",
    "'''\n",
    "\n",
    "print(\"Ferramentas para produção em PyTorch:\")\n",
    "print(\"1. TorchScript - Otimização e portabilidade\")\n",
    "print(\"2. ONNX - Interoperabilidade com outros frameworks\")\n",
    "print(\"3. TorchServe - Servidor de inferência\")\n",
    "print(\"4. C++ Frontend - Para ambientes sem Python\")\n",
    "print(\"5. Mobile - PyTorch para dispositivos móveis\")\n",
    "print(\"6. Quantização - Redução de precisão para eficiência\")\n",
    "print(\"7. Pruning - Remoção de conexões desnecessárias\")\n",
    "\n",
    "print(\"\\nOtimização de desempenho:\")\n",
    "print(\"1. Usar DataLoader com num_workers > 0 e pin_memory=True\")\n",
    "print(\"2. Mover modelos e dados para GPU quando disponível\")\n",
    "print(\"3. Usar tamanhos de batch adequados\")\n",
    "print(\"4. Aplicar técnicas de mixed precision quando possível\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Recursos Adicionais\n",
    "\n",
    "Para aprofundar seus conhecimentos em PyTorch:\n",
    "\n",
    "1. **Documentação Oficial**: [pytorch.org/docs](https://pytorch.org/docs/stable/index.html)\n",
    "2. **Tutoriais Oficiais**: [pytorch.org/tutorials](https://pytorch.org/tutorials/)\n",
    "3. **Fórum PyTorch**: [discuss.pytorch.org](https://discuss.pytorch.org/)\n",
    "4. **GitHub**: [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)\n",
    "5. **Ecossistema PyTorch**:\n",
    "   - **torchvision**: Para visão computacional\n",
    "   - **torchaudio**: Para processamento de áudio\n",
    "   - **torchtext**: Para processamento de texto\n",
    "   - **PyTorch Lightning**: Para simplificar o código de treinamento\n",
    "   - **Hugging Face Transformers**: Para modelos de NLP estado-da-arte\n",
    "   - **Captum**: Para interpretabilidade de modelos\n",
    "   - **TorchServe**: Para servir modelos em produção\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Este tutorial cobriu os conceitos e APIs principais do PyTorch, desde os fundamentos até tópicos avançados. O PyTorch é uma biblioteca poderosa e flexível para aprendizagem de máquina, com uma API intuitiva e um ecossistema rico. Com o conhecimento adquirido neste tutorial, você está pronto para explorar mais a fundo e aplicar o PyTorch em seus próprios projetos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
