{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de TensorFlow com Keras: Conceitos e APIs Principais\n",
    "\n",
    "Este tutorial oferece uma visão geral concisa mas completa do TensorFlow com Keras, uma das combinações de bibliotecas mais populares para aprendizagem de máquina e redes neurais. Vamos explorar os componentes principais, APIs e exemplos práticos para tarefas comuns.\n",
    "\n",
    "## Conteúdo\n",
    "1. [Introdução ao TensorFlow e Keras](#1-introdução-ao-tensorflow-e-keras)\n",
    "2. [Tensores: O Bloco Fundamental](#2-tensores-o-bloco-fundamental)\n",
    "3. [API Keras: Visão Geral](#3-api-keras-visão-geral)\n",
    "4. [Construindo Modelos](#4-construindo-modelos)\n",
    "5. [Camadas e Ativações](#5-camadas-e-ativações)\n",
    "6. [Otimizadores e Funções de Perda](#6-otimizadores-e-funções-de-perda)\n",
    "7. [Carregamento e Processamento de Dados](#7-carregamento-e-processamento-de-dados)\n",
    "8. [Treinamento e Avaliação de Modelos](#8-treinamento-e-avaliação-de-modelos)\n",
    "9. [Callbacks e TensorBoard](#9-callbacks-e-tensorboard)\n",
    "10. [Salvando e Carregando Modelos](#10-salvando-e-carregando-modelos)\n",
    "11. [Redes Neurais Convolucionais (CNNs)](#11-redes-neurais-convolucionais-cnns)\n",
    "12. [Redes Neurais Recorrentes (RNNs)](#12-redes-neurais-recorrentes-rnns)\n",
    "13. [Processamento de Linguagem Natural](#13-processamento-de-linguagem-natural)\n",
    "14. [Transferência de aprendizagem](#14-transferência-de-aprendizagem)\n",
    "15. [TensorFlow para Produção](#15-tensorflow-para-produção)\n",
    "16. [Recursos Adicionais](#16-recursos-adicionais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução ao TensorFlow e Keras\n",
    "\n",
    "**TensorFlow** é uma biblioteca de código aberto desenvolvida pelo Google para computação numérica e aprendizagem de máquina. **Keras** é uma API de alto nível que roda sobre o TensorFlow, simplificando a construção de redes neurais.\n",
    "\n",
    "Principais características:\n",
    "\n",
    "- **TensorFlow**:\n",
    "  - Computação de tensores com suporte a GPU e TPU\n",
    "  - Grafos computacionais para otimização\n",
    "  - Ecossistema rico para pesquisa e produção\n",
    "  - TensorFlow 2.x com execução eager (imperativa) por padrão\n",
    "\n",
    "- **Keras**:\n",
    "  - API de alto nível, fácil de usar\n",
    "  - Desenvolvimento rápido de protótipos\n",
    "  - Suporte para diferentes tipos de redes neurais\n",
    "  - Integrado ao TensorFlow desde a versão 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação do TensorFlow (descomente se necessário)\n",
    "# !pip install tensorflow\n",
    "\n",
    "# Importações básicas\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, losses, metrics, datasets\n",
    "import numpy as np\n",
    "\n",
    "# Verificar versão e disponibilidade de GPU\n",
    "print(f\"TensorFlow versão: {tf.__version__}\")\n",
    "print(f\"Keras versão: {keras.__version__}\")\n",
    "print(f\"GPUs disponíveis: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Configuração para usar GPU se disponível\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Permitir crescimento de memória conforme necessário\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Usando GPU com alocação de memória dinâmica\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Erro na configuração de GPU: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensores: O Bloco Fundamental\n",
    "\n",
    "Assim como no PyTorch, os tensores são as estruturas de dados fundamentais no TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando tensores\n",
    "# 1. A partir de arrays NumPy ou listas Python\n",
    "x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Tensor de lista: \\n{x}\\n\")\n",
    "\n",
    "# 2. Tensores preenchidos\n",
    "zeros = tf.zeros((2, 3))  # Tensor 2x3 de zeros\n",
    "ones = tf.ones((2, 3))    # Tensor 2x3 de uns\n",
    "rand = tf.random.uniform((2, 3))  # Tensor 2x3 com valores aleatórios entre 0 e 1\n",
    "print(f\"Zeros: \\n{zeros}\\n\")\n",
    "print(f\"Ones: \\n{ones}\\n\")\n",
    "print(f\"Random: \\n{rand}\\n\")\n",
    "\n",
    "# 3. Tensores com valores específicos\n",
    "range_tensor = tf.range(0, 10, 2)  # Valores de 0 a 10 com passo 2\n",
    "linspace = tf.linspace(0.0, 10.0, 5)  # 5 valores igualmente espaçados entre 0 e 10\n",
    "print(f\"Range: {range_tensor}\")\n",
    "print(f\"Linspace: {linspace}\\n\")\n",
    "\n",
    "# Propriedades dos tensores\n",
    "print(f\"Forma (shape): {x.shape}\")\n",
    "print(f\"Tipo de dados: {x.dtype}\")\n",
    "print(f\"Dispositivo: {x.device}\\n\")\n",
    "\n",
    "# Operações com tensores\n",
    "a = tf.constant([1, 2, 3])\n",
    "b = tf.constant([4, 5, 6])\n",
    "\n",
    "# Operações aritméticas\n",
    "print(f\"a + b: {a + b}\")\n",
    "print(f\"a * b: {a * b}\")\n",
    "print(f\"Produto escalar: {tf.tensordot(a, b, axes=1)}\\n\")\n",
    "\n",
    "# Redimensionamento\n",
    "c = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"Original: \\n{c}\")\n",
    "print(f\"Reshape: \\n{tf.reshape(c, (3, 2))}\")\n",
    "\n",
    "# Operações de álgebra linear\n",
    "matrix1 = tf.constant([[1, 2], [3, 4]])\n",
    "matrix2 = tf.constant([[5, 6], [7, 8]])\n",
    "matrix_product = tf.matmul(matrix1, matrix2)\n",
    "print(f\"\\nProduto matricial: \\n{matrix_product}\")\n",
    "\n",
    "# Operações de redução\n",
    "print(f\"\\nSoma de todos os elementos: {tf.reduce_sum(c)}\")\n",
    "print(f\"Média por linha: {tf.reduce_mean(c, axis=1)}\")\n",
    "print(f\"Máximo por coluna: {tf.reduce_max(c, axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. API Keras: Visão Geral\n",
    "\n",
    "Keras é a API de alto nível do TensorFlow para construir e treinar modelos de aprendizagem profundo. Existem três maneiras principais de usar Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. API Sequencial (para redes com fluxo linear)\n",
    "sequential_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"Modelo Sequencial:\")\n",
    "sequential_model.summary()\n",
    "\n",
    "# 2. API Funcional (para redes com topologias mais complexas)\n",
    "inputs = keras.Input(shape=(10,))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "functional_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(\"\\nModelo Funcional:\")\n",
    "functional_model.summary()\n",
    "\n",
    "# 3. Subclasse de Model (para lógica personalizada)\n",
    "class CustomModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.dense2 = layers.Dense(32, activation='relu')\n",
    "        self.dense3 = layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "\n",
    "subclass_model = CustomModel()\n",
    "# Construir o modelo com uma chamada de exemplo\n",
    "subclass_model.build((None, 10))\n",
    "\n",
    "print(\"\\nModelo por Subclasse:\")\n",
    "subclass_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construindo Modelos\n",
    "\n",
    "Vamos explorar mais detalhadamente como construir modelos com Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Sequencial com mais detalhes\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(10,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Modelo Sequencial Detalhado:\")\n",
    "model.summary()\n",
    "\n",
    "# Modelo Funcional com múltiplas entradas e saídas\n",
    "# Primeira entrada e seu caminho\n",
    "input_1 = keras.Input(shape=(10,), name='input_1')\n",
    "x1 = layers.Dense(32, activation='relu')(input_1)\n",
    "\n",
    "# Segunda entrada e seu caminho\n",
    "input_2 = keras.Input(shape=(5,), name='input_2')\n",
    "x2 = layers.Dense(16, activation='relu')(input_2)\n",
    "\n",
    "# Concatenando as duas entradas\n",
    "concat = layers.Concatenate()([x1, x2])\n",
    "x = layers.Dense(32, activation='relu')(concat)\n",
    "\n",
    "# Múltiplas saídas\n",
    "output_1 = layers.Dense(1, activation='sigmoid', name='binary_output')(x)\n",
    "output_2 = layers.Dense(5, activation='softmax', name='multi_output')(x)\n",
    "\n",
    "# Criando o modelo com múltiplas entradas e saídas\n",
    "multi_io_model = keras.Model(\n",
    "    inputs=[input_1, input_2],\n",
    "    outputs=[output_1, output_2]\n",
    ")\n",
    "\n",
    "# Compilando com múltiplas perdas e métricas\n",
    "multi_io_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'binary_output': 'binary_crossentropy',\n",
    "        'multi_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'binary_output': 'accuracy',\n",
    "        'multi_output': ['accuracy', 'categorical_accuracy']\n",
    "    },\n",
    "    loss_weights={\n",
    "        'binary_output': 1.0,\n",
    "        'multi_output': 0.5\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nModelo com Múltiplas Entradas e Saídas:\")\n",
    "multi_io_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Camadas e Ativações\n",
    "\n",
    "O Keras oferece uma ampla variedade de camadas e funções de ativação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais tipos de camadas\n",
    "print(\"Camadas Básicas:\")\n",
    "print(\"1. Dense (Totalmente Conectada)\")\n",
    "dense = layers.Dense(32, activation='relu', input_shape=(10,))\n",
    "print(f\"   {dense}\")\n",
    "\n",
    "print(\"\\n2. Dropout (Regularização)\")\n",
    "dropout = layers.Dropout(0.5)\n",
    "print(f\"   {dropout}\")\n",
    "\n",
    "print(\"\\n3. BatchNormalization\")\n",
    "batch_norm = layers.BatchNormalization()\n",
    "print(f\"   {batch_norm}\")\n",
    "\n",
    "print(\"\\nCamadas Convolucionais:\")\n",
    "print(\"1. Conv2D (Convolução 2D)\")\n",
    "conv2d = layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')\n",
    "print(f\"   {conv2d}\")\n",
    "\n",
    "print(\"\\n2. MaxPooling2D\")\n",
    "max_pool = layers.MaxPooling2D(pool_size=(2, 2))\n",
    "print(f\"   {max_pool}\")\n",
    "\n",
    "print(\"\\n3. Conv1D (para sequências)\")\n",
    "conv1d = layers.Conv1D(32, kernel_size=3, activation='relu')\n",
    "print(f\"   {conv1d}\")\n",
    "\n",
    "print(\"\\nCamadas Recorrentes:\")\n",
    "print(\"1. SimpleRNN\")\n",
    "simple_rnn = layers.SimpleRNN(32, return_sequences=True)\n",
    "print(f\"   {simple_rnn}\")\n",
    "\n",
    "print(\"\\n2. LSTM\")\n",
    "lstm = layers.LSTM(32)\n",
    "print(f\"   {lstm}\")\n",
    "\n",
    "print(\"\\n3. GRU\")\n",
    "gru = layers.GRU(32)\n",
    "print(f\"   {gru}\")\n",
    "\n",
    "print(\"\\nCamadas de Reshaping:\")\n",
    "print(\"1. Flatten\")\n",
    "flatten = layers.Flatten()\n",
    "print(f\"   {flatten}\")\n",
    "\n",
    "print(\"\\n2. Reshape\")\n",
    "reshape = layers.Reshape((4, 8))\n",
    "print(f\"   {reshape}\")\n",
    "\n",
    "print(\"\\nCamadas de Merge:\")\n",
    "print(\"1. Concatenate\")\n",
    "concat = layers.Concatenate()\n",
    "print(f\"   {concat}\")\n",
    "\n",
    "print(\"\\n2. Add\")\n",
    "add = layers.Add()\n",
    "print(f\"   {add}\")\n",
    "\n",
    "print(\"\\nFunções de Ativação:\")\n",
    "activations = [\n",
    "    'relu', 'sigmoid', 'tanh', 'softmax', 'elu', 'selu', 'softplus', 'softsign',\n",
    "    'leaky_relu', 'hard_sigmoid', 'exponential', 'linear'\n",
    "]\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"- {activation}\")\n",
    "\n",
    "# Exemplo de uso de diferentes ativações\n",
    "activation_model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(32, activation='tanh'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Também é possível usar a camada de Activation separadamente\n",
    "separate_activation_model = keras.Sequential([\n",
    "    layers.Dense(32, input_shape=(10,)),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(32),\n",
    "    layers.Activation('tanh'),\n",
    "    layers.Dense(10),\n",
    "    layers.Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Otimizadores e Funções de Perda\n",
    "\n",
    "O Keras oferece vários otimizadores e funções de perda para treinar redes neurais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otimizadores\n",
    "print(\"Otimizadores Disponíveis:\")\n",
    "\n",
    "# 1. SGD (Stochastic Gradient Descent)\n",
    "sgd = optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "print(f\"1. SGD: {sgd}\")\n",
    "\n",
    "# 2. Adam (Adaptive Moment Estimation)\n",
    "adam = optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "print(f\"2. Adam: {adam}\")\n",
    "\n",
    "# 3. RMSprop\n",
    "rmsprop = optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "print(f\"3. RMSprop: {rmsprop}\")\n",
    "\n",
    "# 4. Adagrad\n",
    "adagrad = optimizers.Adagrad(learning_rate=0.01)\n",
    "print(f\"4. Adagrad: {adagrad}\")\n",
    "\n",
    "# 5. Adadelta\n",
    "adadelta = optimizers.Adadelta(learning_rate=1.0, rho=0.95)\n",
    "print(f\"5. Adadelta: {adadelta}\")\n",
    "\n",
    "# 6. Adamax\n",
    "adamax = optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
    "print(f\"6. Adamax: {adamax}\")\n",
    "\n",
    "# 7. Nadam\n",
    "nadam = optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
    "print(f\"7. Nadam: {nadam}\")\n",
    "\n",
    "# Funções de Perda\n",
    "print(\"\\nFunções de Perda:\")\n",
    "\n",
    "# Para classificação\n",
    "print(\"Para Classificação:\")\n",
    "print(\"1. binary_crossentropy - Para classificação binária\")\n",
    "print(\"2. categorical_crossentropy - Para classificação multiclasse com one-hot encoding\")\n",
    "print(\"3. sparse_categorical_crossentropy - Para classificação multiclasse com rótulos inteiros\")\n",
    "print(\"4. hinge - Para classificação com margem (SVM)\")\n",
    "\n",
    "# Para regressão\n",
    "print(\"\\nPara Regressão:\")\n",
    "print(\"1. mean_squared_error (MSE) - Erro Quadrático Médio\")\n",
    "print(\"2. mean_absolute_error (MAE) - Erro Absoluto Médio\")\n",
    "print(\"3. mean_absolute_percentage_error (MAPE) - Erro Percentual Absoluto Médio\")\n",
    "print(\"4. huber_loss - Menos sensível a outliers que MSE\")\n",
    "\n",
    "# Exemplo de uso de otimizador e função de perda\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilando com otimizador e função de perda\n",
    "model.compile(\n",
    "    optimizer=adam,  # Usando o otimizador Adam definido acima\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Também é possível usar funções de perda personalizadas\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_true - y_pred) * tf.exp(y_true))\n",
    "\n",
    "# Compilando com função de perda personalizada\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=custom_loss,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Carregamento e Processamento de Dados\n",
    "\n",
    "O TensorFlow oferece várias ferramentas para carregar e processar dados de forma eficiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando tf.data.Dataset para criar pipelines de dados eficientes\n",
    "# 1. Criando um dataset a partir de tensores\n",
    "features = np.random.normal(size=(1000, 10)).astype(np.float32)\n",
    "labels = np.random.randint(0, 2, size=(1000, 1)).astype(np.float32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "print(f\"Dataset: {dataset}\")\n",
    "\n",
    "# 2. Transformando o dataset\n",
    "# Embaralhar, agrupar em batches e pré-buscar\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "print(f\"Dataset transformado: {dataset}\")\n",
    "\n",
    "# 3. Iterando sobre o dataset\n",
    "for features_batch, labels_batch in dataset.take(2):  # Pegando apenas 2 batches para exemplo\n",
    "    print(f\"Batch de features: {features_batch.shape}\")\n",
    "    print(f\"Batch de labels: {labels_batch.shape}\")\n",
    "\n",
    "# 4. Aplicando transformações aos dados\n",
    "def preprocess_data(features, label):\n",
    "    # Normalização\n",
    "    features = (features - tf.reduce_mean(features)) / tf.math.reduce_std(features)\n",
    "    # Adicionando ruído para data augmentation\n",
    "    features = features + tf.random.normal(shape=tf.shape(features), stddev=0.1)\n",
    "    return features, label\n",
    "\n",
    "transformed_dataset = dataset.map(preprocess_data)\n",
    "\n",
    "# 5. Dividindo em conjuntos de treino e validação\n",
    "train_size = int(0.8 * 1000)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((features[:train_size], labels[:train_size]))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((features[train_size:], labels[train_size:]))\n",
    "\n",
    "# Preparando para treinamento\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"\\nDataset de treino: {train_dataset}\")\n",
    "print(f\"Dataset de validação: {val_dataset}\")\n",
    "\n",
    "# 6. Usando datasets integrados do Keras\n",
    "'''\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Pré-processamento\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Criando datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Treinamento e Avaliação de Modelos\n",
    "\n",
    "Vamos ver como treinar e avaliar um modelo com Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo simples para classificação binária\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Treinando o modelo com os datasets criados anteriormente\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset,\n",
    "    verbose=2  # 0: silencioso, 1: barra de progresso, 2: uma linha por época\n",
    ")\n",
    "\n",
    "# Avaliando o modelo\n",
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f\"\\nPerda de validação: {loss:.4f}\")\n",
    "print(f\"Acurácia de validação: {accuracy:.4f}\")\n",
    "\n",
    "# Fazendo previsões\n",
    "sample_data = np.random.normal(size=(5, 10)).astype(np.float32)\n",
    "predictions = model.predict(sample_data)\n",
    "print(f\"\\nPrevisões para 5 amostras: {predictions.flatten()}\")\n",
    "\n",
    "# Visualizando o histórico de treinamento\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gráfico de perda\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Treino')\n",
    "plt.plot(history.history['val_loss'], label='Validação')\n",
    "plt.title('Perda de Treinamento e Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de acurácia\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Treino')\n",
    "plt.plot(history.history['val_accuracy'], label='Validação')\n",
    "plt.title('Acurácia de Treinamento e Validação')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Callbacks e TensorBoard\n",
    "\n",
    "Os callbacks do Keras permitem personalizar o processo de treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principais callbacks\n",
    "print(\"Callbacks Disponíveis:\")\n",
    "\n",
    "# 1. ModelCheckpoint - Salva o modelo durante o treinamento\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "print(\"1. ModelCheckpoint - Salva o modelo durante o treinamento\")\n",
    "\n",
    "# 2. EarlyStopping - Para o treinamento quando não há melhoria\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "print(\"2. EarlyStopping - Para o treinamento quando não há melhoria\")\n",
    "\n",
    "# 3. ReduceLROnPlateau - Reduz a taxa de aprendizagem quando a métrica estagna\n",
    "reduce_lr_cb = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=3,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "print(\"3. ReduceLROnPlateau - Reduz a taxa de aprendizagem quando a métrica estagna\")\n",
    "\n",
    "# 4. TensorBoard - Para visualização e monitoramento\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True\n",
    ")\n",
    "print(\"4. TensorBoard - Para visualização e monitoramento\")\n",
    "\n",
    "# 5. CSVLogger - Registra métricas em um arquivo CSV\n",
    "csv_logger_cb = keras.callbacks.CSVLogger('training.log')\n",
    "print(\"5. CSVLogger - Registra métricas em um arquivo CSV\")\n",
    "\n",
    "# 6. LambdaCallback - Para callbacks personalizados\n",
    "lambda_cb = keras.callbacks.LambdaCallback(\n",
    "    on_epoch_end=lambda epoch, logs: print(f\"Época {epoch+1}: perda = {logs['loss']:.4f}\")\n",
    ")\n",
    "print(\"6. LambdaCallback - Para callbacks personalizados\")\n",
    "\n",
    "# Criando um callback personalizado\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"Iniciando treinamento!\")\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Época {epoch+1}: acurácia = {logs['accuracy']:.4f}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"Treinamento concluído!\")\n",
    "\n",
    "custom_cb = CustomCallback()\n",
    "print(\"7. Callback Personalizado - Implementando a classe Callback\")\n",
    "\n",
    "# Usando callbacks no treinamento\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinando com callbacks\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        checkpoint_cb,\n",
    "        early_stopping_cb,\n",
    "        reduce_lr_cb,\n",
    "        custom_cb\n",
    "    ],\n",
    "    verbose=0  # Desativando a saída padrão para ver apenas os callbacks\n",
    ")\n",
    "\n",
    "# TensorBoard\n",
    "print(\"\\nUsando TensorBoard:\")\n",
    "print(\"1. Crie um callback TensorBoard e adicione-o à lista de callbacks\")\n",
    "print(\"2. Execute o treinamento\")\n",
    "print(\"3. Inicie o servidor TensorBoard: tensorboard --logdir=./logs\")\n",
    "print(\"4. Acesse http://localhost:6006 no navegador\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Salvando e Carregando Modelos\n",
    "\n",
    "O Keras oferece várias maneiras de salvar e carregar modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um modelo simples para demonstração\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 1. Salvando o modelo completo (arquitetura + pesos + estado do otimizador)\n",
    "model.save('modelo_completo.h5')  # Formato HDF5\n",
    "model.save('modelo_completo')     # Formato SavedModel (diretório)\n",
    "\n",
    "# 2. Salvando apenas os pesos\n",
    "model.save_weights('pesos_modelo.h5')\n",
    "\n",
    "# 3. Salvando apenas a arquitetura (como JSON ou YAML)\n",
    "model_json = model.to_json()\n",
    "with open('arquitetura_modelo.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Carregando modelos\n",
    "# 1. Carregando modelo completo\n",
    "loaded_model = keras.models.load_model('modelo_completo.h5')\n",
    "print(\"Modelo carregado:\")\n",
    "loaded_model.summary()\n",
    "\n",
    "# 2. Carregando arquitetura e pesos separadamente\n",
    "# Primeiro, carregamos a arquitetura\n",
    "with open('arquitetura_modelo.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "    \n",
    "model_from_json = keras.models.model_from_json(loaded_model_json)\n",
    "\n",
    "# Depois, carregamos os pesos\n",
    "model_from_json.load_weights('pesos_modelo.h5')\n",
    "\n",
    "# Compilando o modelo carregado\n",
    "model_from_json.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\\nModelo carregado a partir de JSON e pesos:\")\n",
    "model_from_json.summary()\n",
    "\n",
    "# 3. Usando TensorFlow SavedModel (recomendado para produção)\n",
    "# Já salvamos acima com model.save('modelo_completo')\n",
    "loaded_saved_model = keras.models.load_model('modelo_completo')\n",
    "\n",
    "print(\"\\nModelo carregado do formato SavedModel:\")\n",
    "loaded_saved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Redes Neurais Convolucionais (CNNs)\n",
    "\n",
    "As CNNs são especialmente eficazes para processamento de imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma CNN simples para classificação de imagens\n",
    "cnn_model = keras.Sequential([\n",
    "    # Camada de entrada\n",
    "    layers.Input(shape=(28, 28, 1)),  # Para imagens MNIST 28x28 com 1 canal\n",
    "    \n",
    "    # Primeiro bloco convolucional\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Segundo bloco convolucional\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Terceiro bloco convolucional\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    \n",
    "    # Achatamento e camadas densas\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # 10 classes para MNIST\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Modelo CNN:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Testando com dados aleatórios\n",
    "dummy_images = np.random.random((10, 28, 28, 1)).astype(np.float32)\n",
    "dummy_labels = np.random.randint(0, 10, size=(10,))\n",
    "\n",
    "# Treinamento rápido para demonstração\n",
    "cnn_model.fit(dummy_images, dummy_labels, epochs=1, verbose=0)\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = cnn_model.predict(dummy_images[:2])\n",
    "print(f\"\\nPrevisões para 2 imagens: {np.argmax(predictions, axis=1)}\")\n",
    "\n",
    "# Arquiteturas CNN populares\n",
    "print(\"\\nArquiteturas CNN Populares Disponíveis no Keras:\")\n",
    "print(\"1. VGG16/VGG19\")\n",
    "print(\"2. ResNet50/ResNet101/ResNet152\")\n",
    "print(\"3. InceptionV3\")\n",
    "print(\"4. MobileNet/MobileNetV2\")\n",
    "print(\"5. DenseNet121/DenseNet169/DenseNet201\")\n",
    "print(\"6. EfficientNetB0-B7\")\n",
    "\n",
    "# Exemplo de uso de uma arquitetura pré-treinada\n",
    "'''\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Carregando o modelo pré-treinado\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelando as camadas do modelo base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Adicionando camadas personalizadas\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # Número de classes\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Redes Neurais Recorrentes (RNNs)\n",
    "\n",
    "As RNNs são projetadas para processar dados sequenciais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo uma RNN simples\n",
    "rnn_model = keras.Sequential([\n",
    "    layers.Input(shape=(20, 10)),  # (timesteps, features)\n",
    "    layers.SimpleRNN(32, return_sequences=True),\n",
    "    layers.SimpleRNN(32),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"Modelo RNN Simples:\")\n",
    "rnn_model.summary()\n",
    "\n",
    "# Definindo uma LSTM\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.Input(shape=(20, 10)),\n",
    "    layers.LSTM(32, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"\\nModelo LSTM:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Definindo uma GRU\n",
    "gru_model = keras.Sequential([\n",
    "    layers.Input(shape=(20, 10)),\n",
    "    layers.GRU(32, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GRU(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"\\nModelo GRU:\")\n",
    "gru_model.summary()\n",
    "\n",
    "# Definindo uma RNN bidirecional\n",
    "bidirectional_model = keras.Sequential([\n",
    "    layers.Input(shape=(20, 10)),\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(\"\\nModelo Bidirecional:\")\n",
    "bidirectional_model.summary()\n",
    "\n",
    "# Testando com dados aleatórios\n",
    "dummy_sequences = np.random.random((10, 20, 10)).astype(np.float32)  # (batch_size, timesteps, features)\n",
    "dummy_labels = np.random.randint(0, 2, size=(10, 1)).astype(np.float32)\n",
    "\n",
    "# Compilando e treinando o modelo LSTM\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(dummy_sequences, dummy_labels, epochs=1, verbose=0)\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = lstm_model.predict(dummy_sequences[:2])\n",
    "print(f\"\\nPrevisões para 2 sequências: {predictions.flatten()}\")\n",
    "\n",
    "print(\"\\nComparação entre RNN, LSTM e GRU:\")\n",
    "print(\"1. RNN: Simples, mas sofre com o problema de gradientes que desaparecem/explodem\")\n",
    "print(\"2. LSTM: Resolve o problema dos gradientes com células de memória e gates\")\n",
    "print(\"3. GRU: Versão simplificada da LSTM, geralmente mais rápida e com desempenho similar\")\n",
    "\n",
    "print(\"\\nAplicações Comuns de RNNs:\")\n",
    "print(\"1. Processamento de Linguagem Natural (NLP)\")\n",
    "print(\"2. Reconhecimento de Fala\")\n",
    "print(\"3. Previsão de Séries Temporais\")\n",
    "print(\"4. Tradução Automática\")\n",
    "print(\"5. Geração de Texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Processamento de Linguagem Natural\n",
    "\n",
    "O TensorFlow e Keras são amplamente utilizados para tarefas de NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de modelo para classificação de texto\n",
    "max_features = 10000  # Tamanho do vocabulário\n",
    "embedding_dim = 128   # Dimensão do embedding\n",
    "sequence_length = 100 # Comprimento máximo da sequência\n",
    "\n",
    "# Modelo de classificação de texto\n",
    "nlp_model = keras.Sequential([\n",
    "    layers.Input(shape=(sequence_length,)),\n",
    "    \n",
    "    # Camada de embedding\n",
    "    layers.Embedding(max_features, embedding_dim),\n",
    "    \n",
    "    # Camadas recorrentes\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    layers.Bidirectional(layers.LSTM(64)),\n",
    "    \n",
    "    # Camadas densas\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # Para classificação binária\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "nlp_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Modelo NLP:\")\n",
    "nlp_model.summary()\n",
    "\n",
    "# Pré-processamento de texto\n",
    "print(\"\\nPré-processamento de Texto com Keras:\")\n",
    "\n",
    "# 1. Tokenização\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "texts = [\n",
    "    'Eu amo aprender sobre inteligência artificial',\n",
    "    'O processamento de linguagem natural é fascinante',\n",
    "    'As redes neurais são poderosas para NLP'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "print(\"Vocabulário:\")\n",
    "print(dict(list(tokenizer.word_index.items())[:10]))  # Mostrando as 10 primeiras palavras\n",
    "\n",
    "# 2. Convertendo textos para sequências\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "print(\"\\nSequências:\")\n",
    "for i, seq in enumerate(sequences):\n",
    "    print(f\"Texto {i+1}: {seq}\")\n",
    "\n",
    "# 3. Padding de sequências\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10, padding='post', truncating='post')\n",
    "print(\"\\nSequências com Padding:\")\n",
    "for i, seq in enumerate(padded_sequences):\n",
    "    print(f\"Texto {i+1}: {seq}\")\n",
    "\n",
    "# Modelo de embedding pré-treinado\n",
    "print(\"\\nUsando Embeddings Pré-treinados:\")\n",
    "print(\"1. GloVe, Word2Vec, FastText são embeddings populares\")\n",
    "print(\"2. Podem ser carregados e usados em modelos Keras\")\n",
    "\n",
    "# Exemplo de como carregar embeddings pré-treinados (comentado para não baixar)\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "# Carregando embeddings GloVe\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Criando matriz de embedding\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < max_features:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Usando a matriz de embedding no modelo\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(max_features, embedding_dim, weights=[embedding_matrix], trainable=False),\n",
    "    layers.LSTM(64),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "'''\n",
    "\n",
    "# Transformers e TensorFlow Hub\n",
    "print(\"\\nTransformers e TensorFlow Hub:\")\n",
    "print(\"1. TensorFlow Hub oferece modelos pré-treinados prontos para uso\")\n",
    "print(\"2. BERT, GPT, T5 e outros transformers estão disponíveis\")\n",
    "print(\"3. Podem ser usados para transfer learning em tarefas de NLP\")\n",
    "\n",
    "# Exemplo de uso do BERT (comentado para não baixar)\n",
    "'''\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "# Carregando o modelo BERT\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# Criando o modelo\n",
    "text_input = layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Usando a saída [CLS] para classificação\n",
    "pooled_output = outputs[\"pooled_output\"]\n",
    "sequence_output = outputs[\"sequence_output\"]\n",
    "\n",
    "# Adicionando camadas para classificação\n",
    "x = layers.Dense(64, activation='relu')(pooled_output)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Criando o modelo final\n",
    "bert_model = keras.Model(inputs=text_input, outputs=x)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Transferência de aprendizagem\n",
    "\n",
    "O TensorFlow facilita o uso de modelos pré-treinados para transferência de aprendizagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de transferência de aprendizagem com modelos pré-treinados\n",
    "'''\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Carregando o modelo base pré-treinado\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Congelando o modelo base\n",
    "base_model.trainable = False\n",
    "\n",
    "# Criando um novo modelo com o modelo base\n",
    "model = keras.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')  # Número de classes\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "'''\n",
    "\n",
    "print(\"Transferência de aprendizagem em TensorFlow/Keras:\")\n",
    "print(\"1. Carregar um modelo pré-treinado (geralmente sem a camada de classificação)\")\n",
    "print(\"2. Congelar as camadas do modelo base (opcional)\")\n",
    "print(\"3. Adicionar novas camadas para a tarefa específica\")\n",
    "print(\"4. Treinar o modelo com taxa de aprendizagem reduzida\")\n",
    "\n",
    "print(\"\\nFine-tuning (Ajuste Fino):\")\n",
    "print(\"1. Treinar inicialmente com o modelo base congelado\")\n",
    "print(\"2. Descongelar algumas camadas superiores do modelo base\")\n",
    "print(\"3. Treinar novamente com taxa de aprendizagem muito baixa\")\n",
    "\n",
    "# Exemplo de código para fine-tuning\n",
    "'''\n",
    "# Primeira fase: treinar com o modelo base congelado\n",
    "base_model.trainable = False\n",
    "model.fit(train_dataset, epochs=10)\n",
    "\n",
    "# Segunda fase: descongelar algumas camadas e treinar com taxa de aprendizagem baixa\n",
    "base_model.trainable = True\n",
    "\n",
    "# Congelar as primeiras camadas e treinar apenas as últimas\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Recompilar com taxa de aprendizagem mais baixa\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # Taxa de aprendizagem muito baixa\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continuar o treinamento\n",
    "model.fit(train_dataset, epochs=5)\n",
    "'''\n",
    "\n",
    "print(\"\\nModelos Pré-treinados Disponíveis:\")\n",
    "print(\"1. Para Visão Computacional: VGG, ResNet, Inception, MobileNet, EfficientNet\")\n",
    "print(\"2. Para NLP: BERT, GPT, T5, Universal Sentence Encoder\")\n",
    "print(\"3. Para Áudio: AudioSet, Speech Commands\")\n",
    "\n",
    "print(\"\\nTensorFlow Hub:\")\n",
    "print(\"1. Repositório de modelos pré-treinados prontos para uso\")\n",
    "print(\"2. Fácil integração com Keras\")\n",
    "print(\"3. Suporte para diferentes domínios e tarefas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. TensorFlow para Produção\n",
    "\n",
    "O TensorFlow oferece ferramentas para otimizar e implementar modelos em produção:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow para Produção:\")\n",
    "\n",
    "print(\"\\n1. TensorFlow SavedModel:\")\n",
    "print(\"   - Formato padrão para salvar modelos TensorFlow\")\n",
    "print(\"   - Contém o grafo computacional completo\")\n",
    "print(\"   - Pode ser carregado em diferentes ambientes\")\n",
    "\n",
    "# Exemplo de salvamento no formato SavedModel\n",
    "'''\n",
    "model.save('saved_model_dir')\n",
    "'''\n",
    "\n",
    "print(\"\\n2. TensorFlow Lite:\")\n",
    "print(\"   - Versão otimizada do TensorFlow para dispositivos móveis e embarcados\")\n",
    "print(\"   - Suporta quantização para reduzir o tamanho do modelo\")\n",
    "print(\"   - Otimizado para latência e tamanho\")\n",
    "\n",
    "# Exemplo de conversão para TensorFlow Lite\n",
    "'''\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_dir')\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Salvando o modelo\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "'''\n",
    "\n",
    "print(\"\\n3. TensorFlow.js:\")\n",
    "print(\"   - Executa modelos TensorFlow no navegador ou Node.js\")\n",
    "print(\"   - Permite inferência no lado do cliente\")\n",
    "print(\"   - Suporta treinamento no navegador\")\n",
    "\n",
    "# Exemplo de conversão para TensorFlow.js\n",
    "'''\n",
    "!tensorflowjs_converter --input_format=keras saved_model_dir tfjs_model\n",
    "'''\n",
    "\n",
    "print(\"\\n4. TensorFlow Serving:\")\n",
    "print(\"   - Sistema de serviço para modelos em produção\")\n",
    "print(\"   - Suporta versionamento e atualização de modelos\")\n",
    "print(\"   - Alta performance e escalabilidade\")\n",
    "\n",
    "print(\"\\n5. Otimização de Modelos:\")\n",
    "print(\"   - Quantização: Redução de precisão (float32 para int8)\")\n",
    "print(\"   - Pruning: Remoção de conexões desnecessárias\")\n",
    "print(\"   - Compressão: Redução do tamanho do modelo\")\n",
    "\n",
    "# Exemplo de quantização\n",
    "'''\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model_dir')\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_tflite_model = converter.convert()\n",
    "'''\n",
    "\n",
    "print(\"\\n6. TensorFlow Extended (TFX):\")\n",
    "print(\"   - Plataforma para ML em produção em grande escala\")\n",
    "print(\"   - Componentes para todo o pipeline de ML\")\n",
    "print(\"   - Validação de dados, treinamento, avaliação, implantação\")\n",
    "\n",
    "print(\"\\n7. Monitoramento e Logging:\")\n",
    "print(\"   - TensorBoard para visualização de métricas\")\n",
    "print(\"   - TensorFlow Profiler para análise de desempenho\")\n",
    "print(\"   - Integração com sistemas de monitoramento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Recursos Adicionais\n",
    "\n",
    "Para aprofundar seus conhecimentos em TensorFlow e Keras:\n",
    "\n",
    "1. **Documentação Oficial**:\n",
    "   - TensorFlow: [tensorflow.org/api_docs](https://www.tensorflow.org/api_docs)\n",
    "   - Keras: [keras.io/api](https://keras.io/api/)\n",
    "\n",
    "2. **Tutoriais Oficiais**:\n",
    "   - [tensorflow.org/tutorials](https://www.tensorflow.org/tutorials)\n",
    "   - [keras.io/guides](https://keras.io/guides/)\n",
    "\n",
    "3. **Cursos Online**:\n",
    "   - [TensorFlow Developer Certificate](https://www.tensorflow.org/certificate)\n",
    "   - [Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)\n",
    "   - [TensorFlow in Practice Specialization (Coursera)](https://www.coursera.org/specializations/tensorflow-in-practice)\n",
    "\n",
    "4. **Livros**:\n",
    "   - \"Deep Learning with Python\" por François Chollet (criador do Keras)\n",
    "   - \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" por Aurélien Géron\n",
    "\n",
    "5. **Comunidade e Fóruns**:\n",
    "   - [Stack Overflow - TensorFlow](https://stackoverflow.com/questions/tagged/tensorflow)\n",
    "   - [TensorFlow Forum](https://discuss.tensorflow.org/)\n",
    "   - [GitHub - TensorFlow](https://github.com/tensorflow/tensorflow)\n",
    "   - [GitHub - Keras](https://github.com/keras-team/keras)\n",
    "\n",
    "6. **Ecossistema TensorFlow**:\n",
    "   - TensorFlow Hub: Repositório de modelos pré-treinados\n",
    "   - TensorFlow Datasets: Conjuntos de dados prontos para uso\n",
    "   - TensorFlow Graphics: Para computação gráfica\n",
    "   - TensorFlow Probability: Para modelagem probabilística\n",
    "   - TensorFlow Federated: Para aprendizagem federado\n",
    "   - TensorFlow Quantum: Para computação quântica\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Este tutorial cobriu os conceitos e APIs principais do TensorFlow com Keras, desde os fundamentos até tópicos avançados. O TensorFlow é uma biblioteca poderosa e flexível para aprendizagem de máquina, com o Keras fornecendo uma API de alto nível intuitiva. Com o conhecimento adquirido neste tutorial, você está pronto para explorar mais a fundo e aplicar o TensorFlow e Keras em seus próprios projetos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
