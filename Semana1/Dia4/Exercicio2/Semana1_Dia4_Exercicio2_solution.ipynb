{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Faebs66OYZ3h"
      },
      "source": [
        "# Exercício: Sentimento em Tweets Portugueses com GRU (PyTorch)\n",
        "* **Descrição:** Classificar tweets PT em positivo/negativo usando GRU bidireccional.\n",
        "* **Dataset:** Portuguese Tweets for Sentiment Analysis (10 k tweets)\n",
        "  * **Mais Informações:** https://www.kaggle.com/datasets/augustop/portuguese-tweets-for-sentiment-analysis?utm_source=chatgpt.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoH8E5--ZOf3"
      },
      "source": [
        "## Passo a passo\n",
        "\n",
        "1. Carregar tweet, sentiment; mapear posit→1, neg→0.\n",
        "2. Tokenizar com spaCy em Português, cortar a 50 tokens.\n",
        "3. Embedding(num_embeddings=20 000, dim=128) + Bidirectional GRU(128).\n",
        "4. Otimizador Adam, BCELoss; epochs = 10, batch = 64 (GPU).\n",
        "5. Medir accuracy & F1 no conjunto de teste; imprimir frases mal-classificadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miGoUMBJm5o1"
      },
      "outputs": [],
      "source": [
        "# Opção de download do dataset\n",
        "\n",
        "# Treino (Train50.csv são 50 mil tweets)\n",
        "# Pode usar também Train100.csv, Train200.csv, Train300.csv, Train400.csv, Train500.csv)\n",
        "!wget https://genaiacademy.s3.eu-west-3.amazonaws.com/tweetspt/TrainingDatasets/Train50.csv\n",
        "\n",
        "# Teste\n",
        "!wget https://genaiacademy.s3.eu-west-3.amazonaws.com/tweetspt/TestDatasets/Test.csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDYhAUP2aFA7"
      },
      "outputs": [],
      "source": [
        "#!pip install torch spacy==3.7.2 pt_core_news_sm\n",
        "import torch, torch.nn as nn\n",
        "import pandas as pd, spacy, random, numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "_WenD1dw1KQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ-QAlYKjE0v"
      },
      "outputs": [],
      "source": [
        "# ---------- 1. carregar e preparar ----------\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# Carregar dados de treino\n",
        "df_train = pd.read_csv('Train50.csv', sep=';')\n",
        "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_train = df_train.head(10000)\n",
        "\n",
        "# Supondo que os sentimentos já vêm como 0 ou 1, verificamos os dados\n",
        "print(f\"Colunas disponíveis: {df_train.columns.tolist()}\")\n",
        "print(f\"Valores únicos para sentiment: {df_train.sentiment.unique()}\")\n",
        "df_train.info()\n",
        "\n",
        "# Ajustar com base nos dados reais\n",
        "# Como já é numérico, apenas copiamos para 'label'\n",
        "df_train['label'] = df_train.sentiment\n",
        "\n",
        "# Carregar dados de teste\n",
        "df_test = pd.read_csv('Test.csv', sep=';')\n",
        "df_test = df_test.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_test = df_test.head(10000)\n",
        "\n",
        "# Mesmo tratamento para os dados de teste\n",
        "df_test['label'] = df_test.sentiment\n",
        "\n",
        "# Identificar a coluna com os textos dos tweets (tweet ou tweet_text)\n",
        "tweet_column = 'tweet_text' if 'tweet_text' in df_train.columns else 'tweet'\n",
        "print(f\"Usando a coluna '{tweet_column}' para os textos dos tweets\")\n",
        "\n",
        "# Exibir exemplos para verificação\n",
        "print(\"\\nExemplos de treino:\")\n",
        "print(df_train[[tweet_column, 'label']].head(5))\n",
        "print(\"\\nExemplos de teste:\")\n",
        "print(df_test[[tweet_column, 'label']].head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6UZpKhyk09P"
      },
      "outputs": [],
      "source": [
        "# ---------- 2. Tokenização e Vocabulário usando apenas core PyTorch ----------\n",
        "\n",
        "# Função de tokenização usando spaCy diretamente\n",
        "def tokenize(text):\n",
        "    return [token.text.lower() for token in nlp(text)]\n",
        "\n",
        "# Construir vocabulário sem usar torchtext\n",
        "class SimpleVocabulary:\n",
        "    def __init__(self, min_freq=2):\n",
        "        self.word2idx = {'<pad>': 0, '<unk>': 1}  # Inicia com tokens especiais\n",
        "        self.idx2word = {0: '<pad>', 1: '<unk>'}\n",
        "        self.word_counts = Counter()\n",
        "        self.min_freq = min_freq\n",
        "\n",
        "    def build_from_texts(self, texts):\n",
        "        # Tokeniza e conta todas as palavras\n",
        "        for text in texts:\n",
        "            tokens = tokenize(text)\n",
        "            self.word_counts.update(tokens)\n",
        "\n",
        "        # Adiciona ao vocabulário apenas palavras que atingem min_freq\n",
        "        idx = len(self.word2idx)\n",
        "        for word, count in self.word_counts.items():\n",
        "            if count >= self.min_freq and word not in self.word2idx:\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "        print(f\"Vocabulário construído com {len(self.word2idx)} palavras únicas\")\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)\n",
        "\n",
        "    def text_to_indices(self, text, max_len=50):\n",
        "        # Converte um texto para sequência de índices\n",
        "        tokens = tokenize(text)[:max_len]  # Trunca para o tamanho máximo\n",
        "        indices = [self.word2idx.get(token, 1) for token in tokens]  # 1 é o índice de <unk>\n",
        "        # Padding para garantir tamanho fixo\n",
        "        return indices + [0] * (max_len - len(indices))\n",
        "\n",
        "# Cria e treina o vocabulário\n",
        "max_len = 50\n",
        "vocab = SimpleVocabulary(min_freq=2).build_from_texts(df_train[tweet_column])\n",
        "\n",
        "# Função auxiliar para converter um lote de textos\n",
        "def texts_to_tensor(texts, max_len=50):\n",
        "    return torch.tensor([vocab.text_to_indices(text, max_len) for text in texts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnkBrpnmmF7Q"
      },
      "outputs": [],
      "source": [
        "# Preparar dados de treino\n",
        "X_train = texts_to_tensor(df_train[tweet_column])\n",
        "y_train = torch.tensor(df_train.label.values)\n",
        "# Embaralhar os dados de treino\n",
        "perm = torch.randperm(len(X_train))\n",
        "X_train, y_train = X_train[perm], y_train[perm]\n",
        "\n",
        "# Preparar dados de teste separados\n",
        "X_test = texts_to_tensor(df_test[tweet_column])\n",
        "y_test = torch.tensor(df_test.label.values)\n",
        "\n",
        "print(f\"Dados de treino: {X_train.shape[0]} exemplos\")\n",
        "print(f\"Dados de teste: {X_test.shape[0]} exemplos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SL0dCnOhk8jD"
      },
      "outputs": [],
      "source": [
        "# ---------- 3. modelo ----------\n",
        "class SentGRU(nn.Module):\n",
        "    def __init__(self, vocab_sz):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_sz, 128, padding_idx=0)\n",
        "        self.gru = nn.GRU(128, 128, bidirectional=True, batch_first=True)\n",
        "        self.fc  = nn.Linear(256, 1)\n",
        "    def forward(self, x):\n",
        "        emb = self.emb(x)\n",
        "        _, h = self.gru(emb)\n",
        "        h = torch.cat((h[0], h[1]), dim=1)\n",
        "        return torch.sigmoid(self.fc(h)).squeeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg_9HfEjk9na"
      },
      "outputs": [],
      "source": [
        "# Usar GPU se estiver disponível\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Usando: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTZDaF39lKm6"
      },
      "outputs": [],
      "source": [
        "model = SentGRU(len(vocab)).to(device)\n",
        "opt, loss_fn = torch.optim.Adam(model.parameters(), 1e-3), nn.BCELoss()\n",
        "\n",
        "# ---------- 4. treino ----------\n",
        "def batch_iter(X, y, bs):\n",
        "    idx = list(range(len(X)))\n",
        "    random.shuffle(idx)\n",
        "    for i in range(0, len(idx), bs):\n",
        "        sel = idx[i:i+bs]\n",
        "        yield X[sel].to(device), y[sel].float().to(device)\n",
        "\n",
        "# Criar uma pequena validação para monitorar durante o treinamento\n",
        "val_size = int(0.1 * len(X_train))\n",
        "X_val, y_val = X_train[-val_size:], y_train[-val_size:]\n",
        "X_train, y_train = X_train[:-val_size], y_train[:-val_size]\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    batches = 0\n",
        "\n",
        "    for xb, yb in batch_iter(X_train, y_train, 64):\n",
        "        opt.zero_grad()\n",
        "        output = model(xb)\n",
        "        loss = loss_fn(output, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "        batches += 1\n",
        "\n",
        "    avg_loss = total_loss / batches\n",
        "\n",
        "    # Validação rápida\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_probs = model(X_val.to(device)).cpu()\n",
        "    val_preds = (val_probs > 0.5).int()\n",
        "    val_acc = accuracy_score(y_val, val_preds)\n",
        "    val_f1 = f1_score(y_val, val_preds)\n",
        "\n",
        "    print(f'Época {epoch+1}: loss={avg_loss:.4f}, val_acc={val_acc:.3f}, val_f1={val_f1:.3f}')\n",
        "\n",
        "# ---------- 5. avaliação no conjunto de teste ----------\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    test_probs = model(X_test.to(device)).cpu()\n",
        "test_preds = (test_probs > 0.5).int()\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "test_prec = precision_score(y_test, test_preds)\n",
        "test_rec = recall_score(y_test, test_preds)\n",
        "test_f1 = f1_score(y_test, test_preds)\n",
        "\n",
        "print(f'\\nResultados finais no conjunto de teste:')\n",
        "print(f'Accuracy: {test_acc:.4f}')\n",
        "print(f'Precision: {test_prec:.4f}')\n",
        "print(f'Recall: {test_rec:.4f}')\n",
        "print(f'F1-score: {test_f1:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kn669dVumf_z"
      },
      "outputs": [],
      "source": [
        "# ---------- 6. frases mal-classificadas ----------\n",
        "errors = (test_preds != y_test).nonzero(as_tuple=True)[0]\n",
        "\n",
        "# Get 5 random error indices\n",
        "num_errors_to_print = min(5, len(errors))  # Ensure we don't try to print more than available\n",
        "random_error_indices = random.sample(range(len(errors)), num_errors_to_print)\n",
        "\n",
        "for i in random_error_indices:\n",
        "    error_index = errors[i].item()  # Get the actual index from the errors tensor\n",
        "    tweet = df_test[tweet_column].iloc[error_index]\n",
        "    true_label = y_test[error_index].item()\n",
        "    pred_label = test_preds[error_index].item()\n",
        "\n",
        "    print(f\"\\nTweet: {tweet}\")\n",
        "    print(f\"Verdadeiro: {'Positivo' if true_label == 1 else 'Negativo'} ({true_label})\")\n",
        "    print(f\"Previsto: {'Positivo' if pred_label == 1 else 'Negativo'} ({pred_label})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Mostrar frases bem classificadas"
      ],
      "metadata": {
        "id": "3RqXAQpq56Zc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}